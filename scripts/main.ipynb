{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edwin/git/Transcendence_LLM/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/edwin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "huggingface_hub.login(os.environ['HF_API_TOKEN'])\n",
    "def load_eval_dataset(dataset_name=\"squad_v2\"):\n",
    "    # Load the SQuAD v2 dataset by default\n",
    "    file_path = \"Meta-Llama-3.1-8B-evals/Details_squad_2024-07-22T14-58-08.291117.parquet.gzip\"\n",
    "    dataset = load_dataset(dataset_name, data_files=file_path, split=\"train\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def generate_llm(prompt: str, model: str = \"llama3.1\", temperature=0.01) -> str:\n",
    "    return ollama.generate(\n",
    "        model=model, prompt=prompt, options=ollama.Options(temperature=temperature, num_predict=32)\n",
    "    )[\"response\"]\n",
    "\n",
    "dataset = load_eval_dataset(dataset_name=\"meta-llama/Meta-Llama-3.1-8B-evals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['task_type', 'task_name', 'subtask_name', 'input_question', 'input_choice_list', 'input_final_prompts', 'input_correct_responses', 'output_prediction_text', 'output_parsed_answer', 'output_choice_completions', 'output_choice_negative_log_likelihoods', 'output_metrics', 'is_correct', 'input_question_hash', 'input_final_prompts_hash', 'benchmark_label', 'eval_config'],\n",
       "    num_rows: 11873\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Texas is Austin.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_llm(\"What is the capital of Texas?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{hi}'.format(hi='hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "############################################\n",
    "# Attempt 3:\n",
    "############################################\n",
    "predictions = defaultdict(list)\n",
    "references = defaultdict(list)\n",
    "\n",
    "\n",
    "eval_prompt = \"\"\" \n",
    "You are a evaluator that needs to compare if response1 matches any of the other_responses to determine if they are the same or not.\n",
    "\n",
    "response1: {llm_response}\n",
    "\n",
    "other_responses: {input_correct_responses}\n",
    "\n",
    "Answer just \"yes\" or \"no\" to the question: \"Do the responses match?\"\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_llm(temp=0.01, total_evals=1):\n",
    "    evaluation_history = []\n",
    "\n",
    "    for i, example in enumerate(dataset):\n",
    "        # print(i, example)\n",
    "        if i == total_evals:\n",
    "            break\n",
    "        \n",
    "        generation = generate_llm(example[\"input_question\"])\n",
    "        print(generation)\n",
    "        print(example[\"input_correct_responses\"])\n",
    "\n",
    "        correct = generate_llm(\n",
    "            eval_prompt.format(\n",
    "                llm_response=generation,\n",
    "                input_correct_responses=example[\"input_correct_responses\"],\n",
    "            )\n",
    "        )\n",
    "        evaluation_history.append(\n",
    "            {\n",
    "                \"id\": example[\"input_question_hash\"],\n",
    "                \"generation\": generation,\n",
    "                \"input_correct_responses\": example[\"input_correct_responses\"],\n",
    "                \"correct\": correct,\n",
    "                \"temperature\": temp,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    return evaluation_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in background. The background only mentions that John Paul II's visits to his native country brought support to the solidarity movement, but it does not mention that he\n",
      "['john paul ii', 'john paul ii', 'john paul ii']\n",
      "Near Ballarat, gold was discovered in 1851.\n",
      "['1851', 'in 1851', '1851']\n",
      "20 million ounces\n",
      "['20 million ounces', '20 million ounces', '20 million ounces']\n",
      "Not in background. The text does not mention anything about a New South Wales president or the issuance of writs for their election. It only talks about Victoria's\n",
      "['not in background']\n",
      "Not in background.\n",
      "['not in background']\n",
      "Here are the answers to each question based on the provided background paragraph:\n",
      "\n",
      "1. What was the name of Beyoncé's first solo album?\n",
      "A: Dangerously\n",
      "['not in background']\n",
      "One of the largest gold rushes the world has ever seen.\n",
      "['gold rush', 'gold rush', 'gold rushes']\n",
      "Based on the background information, here are the answers to your questions:\n",
      "\n",
      "Q: On what date was Victoria declared independent from New South Wales?\n",
      "A: 1\n",
      "['sevenfold', 'sevenfold', '76000 to 540000']\n",
      "Not in background.\n",
      "['not in background']\n",
      "Not in background.\n",
      "['not in background']\n",
      "I'll answer each question based on the provided background information.\n",
      "\n",
      "1. By August 2010 how many public schools did Victoria have?\n",
      "A: 1,\n",
      "['not in background']\n",
      "Not in background.\n",
      "['not in background']\n",
      "Victoria\n",
      "['victoria', 'victoria', 'victoria']\n",
      "Not in background. The information provided only mentions that Victoria is the centre of dairy farming in Australia, but does not provide a specific number for the total number of\n",
      "['3 million', '3 million', '3 million']\n",
      "60%\n",
      "['60', '60', '60']\n",
      "Two-thirds.\n",
      "['twothirds', 'nearly twothirds', 'twothirds']\n",
      "To Asia.\n",
      "['asia', 'asia', 'asia']\n",
      "Not in background.\n",
      "['not in background']\n",
      "Not in background.\n",
      "['not in background']\n",
      "Not in background.\n",
      "['not in background']\n",
      "Not in background.\n",
      "['not in background']\n",
      "Based on the provided background, here are the answers:\n",
      "\n",
      "Q: To where is most of the milk and beef from Victoria exported to?\n",
      "A: Not in background\n",
      "['not in background']\n",
      "1,600 mm (5 ft 3 in) broad gauge.\n",
      "['1600 mm', '1600 mm 5 ft 3 in broad gauge', '1600 mm 5 ft 3 in broad gauge']\n",
      "1,435 mm (4 ft 8 1⁄2 in) standard gauge.\n",
      "['1435 mm', '1435 mm 4 ft 8 1⁄2 in standard gauge', '1435 mm 4 ft 8 1⁄2 in standard gauge']\n",
      "Here are the answers to each question based on the provided background:\n",
      "\n",
      "1. The complexity of problems often depends on what?\n",
      "Answer: the type of reduction being used\n",
      "['not in background']\n",
      "Not in background (the background paragraph does not mention anything about the hardest problems in NP)\n",
      "['npcomplete', 'npcomplete', 'npcomplete']\n",
      "Based on the background, I'll answer your questions:\n",
      "\n",
      "Q: The hardest problems in NP can be analogously written as what class of problems?\n",
      "A: NP\n",
      "['np', 'p', 'p']\n",
      "Here are the answers to each question:\n",
      "\n",
      "1. Q: On what date was Victoria declared independent from New South Wales?\n",
      "A: 1 July 1851\n",
      "['not in background']\n",
      "Here are the answers to each question:\n",
      "\n",
      "1. What was the name of Beyoncé's first solo album?\n",
      "A: Dangerously in Love\n",
      "\n",
      "2. On\n",
      "['not in background']\n",
      "1,548\n",
      "['1548', '1548', '1548']\n",
      "489\n",
      "['489', '489', '489']\n",
      "540,800\n",
      "['540800', '540800', '540800']\n",
      "63,519\n",
      "['63519', '63519', '63519']\n",
      "61%\n",
      "['61', '61', '61']\n",
      "Not in background. The background only mentions public schools, Catholic schools, and independent schools, but does not mention trade schools.\n",
      "['not in background']\n",
      "Based on the provided background information, here are the answers to each question:\n",
      "\n",
      "1. The hardest problems in NP can be analogously written as what class of problems\n",
      "['not in background']\n",
      "Here are the answers to each question based on the provided background:\n",
      "\n",
      "Q: The hardest problems in NP can be analogously written as what class of problems?\n",
      "A\n",
      "['not in background']\n",
      "P\n",
      "['p', 'p', 'p']\n",
      "The Cobham–Edmonds thesis.\n",
      "['cobham–edmonds thesis', 'cobham–edmonds thesis', 'cobham–edmonds thesis']\n",
      "Based on the background, here are the answers:\n",
      "\n",
      "Q: What complexity class is commonly characterized by unknown algorithms to enhance solvability?\n",
      "A: NP\n",
      "['np', 'np', 'np']\n",
      "Here are the answers:\n",
      "\n",
      "1. What was the name of Beyoncé's first solo album?\n",
      "A: Dangerously in Love\n",
      "\n",
      "2. What complexity class is\n",
      "['boolean satisfiability problem', 'boolean satisfiability problem']\n",
      "Here are the answers to each question based on the background information:\n",
      "\n",
      "1. What was the name of Beyoncé's first solo album?\n",
      "A: Dangerously in\n",
      "['turing machines', 'deterministic turing machines', 'deterministic turing machines']\n",
      "Here are the answers to each question based on the background information:\n",
      "\n",
      "1. What was the name of Beyoncé's first solo album?\n",
      "A: Dangerously in\n",
      "['not in background']\n",
      "Here are the answers based on the background paragraph:\n",
      "\n",
      "Q: The hardest problems in NP can be analogously written as what class of problems?\n",
      "A: NP-complete\n",
      "['there is no known polynomialtime solution', 'no known polynomialtime solution', 'there is no known polynomialtime solution']\n",
      "Here are the answers based on the background paragraph:\n",
      "\n",
      "Q: The hardest problems in NP can be analogously written as what class of problems?\n",
      "A: NP-complete\n",
      "['np', 'np', 'np']\n",
      "Not in background. \n",
      "\n",
      "The background paragraph doesn't mention anything about problems being \"soft\" for a class C. It only talks about problems being \"hard\"\n",
      "['not in background']\n",
      "Here are the answers based on the provided background:\n",
      "\n",
      "1. The hardest problems in NP can be analogously written as what class of problems?\n",
      "A: NP-complete\n",
      "['not in background']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_evaluations ={}\n",
    "for temp in [0.001, 1.0]:\n",
    "    evaluation_history = evaluate_llm(temp, total_evals=100)\n",
    "    all_evaluations[temp] = evaluation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.001: [{'id': '8bda73a1a16177a7b1a27cd99ca72b17ba96f04c86c36a256e2abea3e1b3eeb1',\n",
       "   'generation': \"Not in background. The background only mentions that John Paul II's visits to his native country brought support to the solidarity movement, but it does not mention that he\",\n",
       "   'input_correct_responses': ['john paul ii', 'john paul ii', 'john paul ii'],\n",
       "   'correct': 'no',\n",
       "   'temperature': 0.001}],\n",
       " 1.0: [{'id': '8bda73a1a16177a7b1a27cd99ca72b17ba96f04c86c36a256e2abea3e1b3eeb1',\n",
       "   'generation': \"Not in background. The background only mentions that John Paul II's visits to his native country brought support to the solidarity movement, but it does not mention that he\",\n",
       "   'input_correct_responses': ['john paul ii', 'john paul ii', 'john paul ii'],\n",
       "   'correct': 'no',\n",
       "   'temperature': 1.0}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>generation</th>\n",
       "      <th>input_correct_responses</th>\n",
       "      <th>correct</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8bda73a1a16177a7b1a27cd99ca72b17ba96f04c86c36a...</td>\n",
       "      <td>Not in background. The background only mention...</td>\n",
       "      <td>[john paul ii, john paul ii, john paul ii]</td>\n",
       "      <td>no</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>07fac974d3c5714d8a990a8b1fdd61838939c613926530...</td>\n",
       "      <td>Near Ballarat, gold was discovered in 1851.</td>\n",
       "      <td>[1851, in 1851, 1851]</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d568ae5d2c9056b505e6ee7c42ccc4d612c2b28d466372...</td>\n",
       "      <td>20 million ounces</td>\n",
       "      <td>[20 million ounces, 20 million ounces, 20 mill...</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9c42a3f7f7debe911ba20717c6ec1ff5176f848eaf17d0...</td>\n",
       "      <td>Not in background. The text does not mention a...</td>\n",
       "      <td>[not in background]</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e5e3bae01a94b7d75508c24c2fde048ad295188d160d4d...</td>\n",
       "      <td>Not in background.</td>\n",
       "      <td>[not in background]</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acacdfc0a52b73874229424510308a8a38c7f39b777a9d...</td>\n",
       "      <td>Here are the answers to each question based on...</td>\n",
       "      <td>[not in background]</td>\n",
       "      <td>no</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>72eee02d679572fbd84963ccad470516503245b477fd26...</td>\n",
       "      <td>One of the largest gold rushes the world has e...</td>\n",
       "      <td>[gold rush, gold rush, gold rushes]</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a9dafec980f3985c28054db83b42ce7903fae6e34f5a19...</td>\n",
       "      <td>Based on the background information, here are ...</td>\n",
       "      <td>[sevenfold, sevenfold, 76000 to 540000]</td>\n",
       "      <td>no</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2b024a6662829e65a535a2f824554d7359ec1e09ccf7f4...</td>\n",
       "      <td>Not in background.</td>\n",
       "      <td>[not in background]</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>84390fbbdedc07ede9547732f5ef0a3516969ed75c5ab3...</td>\n",
       "      <td>Not in background.</td>\n",
       "      <td>[not in background]</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  8bda73a1a16177a7b1a27cd99ca72b17ba96f04c86c36a...   \n",
       "1  07fac974d3c5714d8a990a8b1fdd61838939c613926530...   \n",
       "2  d568ae5d2c9056b505e6ee7c42ccc4d612c2b28d466372...   \n",
       "3  9c42a3f7f7debe911ba20717c6ec1ff5176f848eaf17d0...   \n",
       "4  e5e3bae01a94b7d75508c24c2fde048ad295188d160d4d...   \n",
       "5  acacdfc0a52b73874229424510308a8a38c7f39b777a9d...   \n",
       "6  72eee02d679572fbd84963ccad470516503245b477fd26...   \n",
       "7  a9dafec980f3985c28054db83b42ce7903fae6e34f5a19...   \n",
       "8  2b024a6662829e65a535a2f824554d7359ec1e09ccf7f4...   \n",
       "9  84390fbbdedc07ede9547732f5ef0a3516969ed75c5ab3...   \n",
       "\n",
       "                                          generation  \\\n",
       "0  Not in background. The background only mention...   \n",
       "1        Near Ballarat, gold was discovered in 1851.   \n",
       "2                                  20 million ounces   \n",
       "3  Not in background. The text does not mention a...   \n",
       "4                                 Not in background.   \n",
       "5  Here are the answers to each question based on...   \n",
       "6  One of the largest gold rushes the world has e...   \n",
       "7  Based on the background information, here are ...   \n",
       "8                                 Not in background.   \n",
       "9                                 Not in background.   \n",
       "\n",
       "                             input_correct_responses correct  temperature  \n",
       "0         [john paul ii, john paul ii, john paul ii]      no        0.001  \n",
       "1                              [1851, in 1851, 1851]    Yes.        0.001  \n",
       "2  [20 million ounces, 20 million ounces, 20 mill...    Yes.        0.001  \n",
       "3                                [not in background]     Yes        0.001  \n",
       "4                                [not in background]     yes        0.001  \n",
       "5                                [not in background]      no        0.001  \n",
       "6                [gold rush, gold rush, gold rushes]    Yes.        0.001  \n",
       "7            [sevenfold, sevenfold, 76000 to 540000]      no        0.001  \n",
       "8                                [not in background]     yes        0.001  \n",
       "9                                [not in background]     yes        0.001  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(all_evaluations[0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>generation</th>\n",
       "      <th>input_correct_responses</th>\n",
       "      <th>correct</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8bda73a1a16177a7b1a27cd99ca72b17ba96f04c86c36a...</td>\n",
       "      <td>Not in background. The background only mention...</td>\n",
       "      <td>[john paul ii, john paul ii, john paul ii]</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>07fac974d3c5714d8a990a8b1fdd61838939c613926530...</td>\n",
       "      <td>Near Ballarat, gold was discovered in 1851.</td>\n",
       "      <td>[1851, in 1851, 1851]</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d568ae5d2c9056b505e6ee7c42ccc4d612c2b28d466372...</td>\n",
       "      <td>20 million ounces</td>\n",
       "      <td>[20 million ounces, 20 million ounces, 20 mill...</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9c42a3f7f7debe911ba20717c6ec1ff5176f848eaf17d0...</td>\n",
       "      <td>Not in background. The text does not mention a...</td>\n",
       "      <td>[not in background]</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e5e3bae01a94b7d75508c24c2fde048ad295188d160d4d...</td>\n",
       "      <td>Not in background.</td>\n",
       "      <td>[not in background]</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acacdfc0a52b73874229424510308a8a38c7f39b777a9d...</td>\n",
       "      <td>Here are the answers to each question based on...</td>\n",
       "      <td>[not in background]</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>72eee02d679572fbd84963ccad470516503245b477fd26...</td>\n",
       "      <td>One of the largest gold rushes the world has e...</td>\n",
       "      <td>[gold rush, gold rush, gold rushes]</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a9dafec980f3985c28054db83b42ce7903fae6e34f5a19...</td>\n",
       "      <td>Based on the background information, here are ...</td>\n",
       "      <td>[sevenfold, sevenfold, 76000 to 540000]</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2b024a6662829e65a535a2f824554d7359ec1e09ccf7f4...</td>\n",
       "      <td>Not in background.</td>\n",
       "      <td>[not in background]</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>84390fbbdedc07ede9547732f5ef0a3516969ed75c5ab3...</td>\n",
       "      <td>Not in background.</td>\n",
       "      <td>[not in background]</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  8bda73a1a16177a7b1a27cd99ca72b17ba96f04c86c36a...   \n",
       "1  07fac974d3c5714d8a990a8b1fdd61838939c613926530...   \n",
       "2  d568ae5d2c9056b505e6ee7c42ccc4d612c2b28d466372...   \n",
       "3  9c42a3f7f7debe911ba20717c6ec1ff5176f848eaf17d0...   \n",
       "4  e5e3bae01a94b7d75508c24c2fde048ad295188d160d4d...   \n",
       "5  acacdfc0a52b73874229424510308a8a38c7f39b777a9d...   \n",
       "6  72eee02d679572fbd84963ccad470516503245b477fd26...   \n",
       "7  a9dafec980f3985c28054db83b42ce7903fae6e34f5a19...   \n",
       "8  2b024a6662829e65a535a2f824554d7359ec1e09ccf7f4...   \n",
       "9  84390fbbdedc07ede9547732f5ef0a3516969ed75c5ab3...   \n",
       "\n",
       "                                          generation  \\\n",
       "0  Not in background. The background only mention...   \n",
       "1        Near Ballarat, gold was discovered in 1851.   \n",
       "2                                  20 million ounces   \n",
       "3  Not in background. The text does not mention a...   \n",
       "4                                 Not in background.   \n",
       "5  Here are the answers to each question based on...   \n",
       "6  One of the largest gold rushes the world has e...   \n",
       "7  Based on the background information, here are ...   \n",
       "8                                 Not in background.   \n",
       "9                                 Not in background.   \n",
       "\n",
       "                             input_correct_responses correct  temperature  \n",
       "0         [john paul ii, john paul ii, john paul ii]      no          1.0  \n",
       "1                              [1851, in 1851, 1851]    Yes.          1.0  \n",
       "2  [20 million ounces, 20 million ounces, 20 mill...    Yes.          1.0  \n",
       "3                                [not in background]     Yes          1.0  \n",
       "4                                [not in background]     yes          1.0  \n",
       "5                                [not in background]      no          1.0  \n",
       "6                [gold rush, gold rush, gold rushes]    Yes.          1.0  \n",
       "7            [sevenfold, sevenfold, 76000 to 540000]      no          1.0  \n",
       "8                                [not in background]     yes          1.0  \n",
       "9                                [not in background]     yes          1.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(all_evaluations[1.0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
